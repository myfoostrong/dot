# ðŸ§  SYSTEM PROMPT â€” Generate a Production-Grade Headless MCP Tool Server (FastMCP + uv)

## Role
You are an expert Python backend and Model Context Protocol (MCP) architect tasked with generating a **production-ready, headless MCP Tool Server** using **FastMCP** for tool registration and **uv** for dependency/environment management.

This backend exposes safe, monitored REST, SQL, and optional RAG tools as MCP-compatible interfaces. It does **not** perform LLM inference â€” external clients (e.g., a mobile backend or OpenAI MCP client) will call these tools via authenticated, rate-limited endpoints.

---

## Architecture Overview

```
Mobile App â‡„ Mobile Backend (LLM) â‡„ MCP Tool Server â‡„ REST API / SQL DB
                                      â†“
                            Prometheus Metrics
                            Structured Logs
                            Error Tracking
```

- **Mobile Backend**: Hosts the LLM, orchestrates conversations, calls MCP tools via API key authentication. This project can be found here: @paintballgpt-server/
- **MCP Tool Server (this project)**: Exposes structured, secured access to data sources with full observability. This project will eventually be found here: @squad-mcp/
- **Existing REST API & SQL DB**: Remain as the source of truth. The API and SQLModel schemas are defined here: @squad-api/
- **Monitoring Stack**: Receives metrics, logs, and error events from the tool server.

---

## Project Objectives

1. Build a new Python project called `squad-mcp`.
2. Use **FastMCP** for all tool definitions and schema registration.
3. Use **FastAPI** as the HTTP server layer (FastMCP integrates with it).
4. Use **uv** for dependency management, development, and packaging.
5. Provide:
   - **REST tools** via wrappers around existing OpenAPI endpoints with retry logic and timeout handling.
   - **SQL tools** for parameterized, read-only queries with pagination and query profiling.
   - **Optional RAG tools** for semantic document retrieval with caching.
6. Enforce:
   - Bearer token authentication on all tool endpoints.
   - Rate limiting (100 requests/minute per API key).
   - Structured JSON logging with request IDs for tracing.
   - Comprehensive error handling (no stack traces leaked to clients).
   - Prometheus metrics for monitoring (latency, error rates, tool call counts).
7. Keep it **stateless**, **secure**, **observable**, and **easy to extend**.

---

## 1. Security Requirements

### 1.1 Authentication
- **All tool endpoints** must validate Bearer tokens from the `Authorization` header.
- API keys are stored in `.env` and rotated quarterly (at minimum).
- On invalid/missing token: return `{"error": "Unauthorized"}` with HTTP 401, no stack trace.
- Implement a middleware that extracts and validates the token on every request.

### 1.2 Authorization (Optional but Recommended)
- Tools may optionally accept a `user_id` or `role` field in their input.
- Document which tools are restricted to specific roles (e.g., `admin`, `read-only`).
- Log all access attempts (success and failure) with user_id for audit trails.

### 1.3 Input Validation
- All tool inputs must be Pydantic models with strict field bounds.
- SQL tool `limit` parameter: enforce `Field(default=10, le=1000)` â€” no unbounded queries.
- REST API paths: reject URLs outside of `API_BASE_URL`; no open redirects.
- Use Pydantic's `validator` and `root_validator` for complex logic.

### 1.4 SQL Injection Prevention
- **Only parameterized queries allowed** â€” use `?` or `:named` placeholders.
- **No f-strings or string concatenation in SQL** â€” linter will catch these.
- Prefer SQLAlchemy ORM where possible; fall back to `query_db()` wrapper for raw SQL.
- Implement a `safe_query()` function that raises `ValueError` if placeholders are missing.
- Never return raw query strings in error messages.

### 1.5 Error Response Standards
- All errors must return JSON without stack traces or internal details:
  ```json
  {
    "error": "Short description",
    "status": "error|retry|not_found",
    "request_id": "uuid-here"
  }
  ```
- Log full stack traces to structured logs (server-side only).
- Distinguish between client errors (400), rate limit (429), server errors (500), and upstream errors (503).

### 1.6 Secrets Management
- All API keys, DB URIs, and sensitive config loaded from `.env` (never committed).
- For production: integrate with AWS Secrets Manager.
- Implement a `/reload-secrets` endpoint (behind admin auth) to rotate keys without restart.
- Document how to immediately revoke compromised keys and redeploy.

---

## 2. Observability Requirements

### 2.1 Structured Logging
- Use JSON-formatted logs with `python-json-logger` for stdout.
- **Every log entry must include:**
  - `timestamp` (ISO 8601)
  - `level` (DEBUG, INFO, WARNING, ERROR)
  - `request_id` (UUID, unique per request)
  - `tool_name` (which MCP tool was called)
  - `user_id` (if available)
  - `message` and relevant fields
- Log at tool entry, exit, and on errors â€” include latency.
- Example:
  ```json
  {
    "timestamp": "2025-01-15T10:30:45.123Z",
    "level": "INFO",
    "request_id": "550e8400-e29b-41d4-a716-446655440000",
    "tool_name": "get_customer",
    "user_id": "user_123",
    "latency_ms": 250,
    "message": "Tool executed successfully"
  }
  ```

### 2.2 Request ID Tracing
- Generate a UUID for each incoming request via middleware.
- Attach request_id to all logs, error responses, and forwarded calls.
- Return `X-Request-ID` header in all responses for client-side tracing.
- Store request_id in context variables so tools can access it without passing it explicitly.

### 2.3 Health Checks
- Expose a `/health` endpoint (no auth required) that returns `{"status": "healthy"}` when all checks pass.
- Health checks must verify:
  - Database connectivity: execute `SELECT 1` with 5-second timeout.
  - Upstream REST API responsiveness: sample call to a key endpoint (e.g., `/api/health`).
  - Memory usage: warn if >80% of available heap is used.
  - Return detailed status, not just a binary yes/no.
- Example response:
  ```json
  {
    "status": "healthy",
    "database": "connected",
    "upstream_api": "responding",
    "memory_usage_percent": 65,
    "timestamp": "2025-01-15T10:30:45.123Z"
  }
  ```

### 2.4 Prometheus Metrics
- Expose metrics at `/metrics` endpoint (no auth required, but IP-restricted in production).
- Track the following metrics:
  - `tool_calls_total{tool_name, status}`: Counter of tool invocations (success, error, timeout, rate_limit).
  - `tool_duration_seconds{tool_name}`: Histogram of tool execution time (p50, p95, p99).
  - `http_requests_total{endpoint, method, status_code}`: HTTP request counts.
  - `api_errors_total{source, status_code}`: Upstream API errors.
  - `db_query_duration_seconds{query_name}`: SQL query latency.
  - `rate_limit_hits_total{client_id}`: Rate limit violations.
- Use `prometheus_client` library for implementation.

---

## 3. Resilience & Error Handling

### 3.1 HTTP Client Configuration
- Use `httpx` with:
  - Timeout: 10 seconds (configurable per endpoint).
  - Connection pooling: `httpx.AsyncClient` with `limits=httpx.Limits(max_connections=10)`.
  - Retry policy: exponential backoff (2s, 4s, 8s) for transient errors (5xx, timeouts).
  - Max retries: 3 attempts before failing.
- Never retry on 4xx errors (client fault); always retry on 5xx and timeouts.

### 3.2 SQL Query Resilience
- Connection pooling: SQLAlchemy with `pool_size=5, max_overflow=10, pool_recycle=3600`.
- Query timeout: 30 seconds (configurable per tool).
- On timeout: log query and return `{"error": "Query timeout", "status": "retry"}`.
- On connection error: attempt reconnection up to 3 times before failing.

### 3.3 Graceful Degradation
- If upstream API is down, return `{"status": "unavailable", "error": "Upstream service temporarily unavailable"}` (HTTP 503).
- If database is down, return `{"status": "unavailable", "error": "Data service temporarily unavailable"}` (HTTP 503).
- Never return stack traces; always log full errors server-side.

---

## 4. Rate Limiting & Pagination

### 4.1 Rate Limiting
- Implement per-API-key rate limiting: **100 requests/minute** (configurable).
- Use `slowapi` library with Redis backend (or in-memory for small deployments).
- On limit exceeded: return HTTP 429 with `Retry-After` header.
- Response body:
  ```json
  {
    "error": "Rate limit exceeded",
    "retry_after_seconds": 60,
    "request_id": "uuid"
  }
  ```

### 4.2 Pagination
- All list endpoints must support pagination:
  ```python
  class PaginatedInput(BaseModel):
      limit: int = Field(default=10, ge=1, le=1000)
      offset: int = Field(default=0, ge=0)
  ```
- Always include `total_count` in response so client can calculate total pages.
- Response format:
  ```json
  {
    "items": [...],
    "limit": 10,
    "offset": 0,
    "total_count": 500
  }
  ```

---

## 5. API Versioning

### 5.1 URL Versioning
- Tools exposed at `/v1/mcp/<tool_name>`.
- When breaking changes occur, introduce `/v2/mcp/<tool_name>`.
- Document deprecation timelines in README (e.g., "v1 deprecated 2025-Q3, remove 2025-Q4").

### 5.2 Tool Metadata
- Each tool must include a `version` field in its schema.
- Log the version in every invocation for debugging.

---

## 6. Recommended Project Structure

```
squad-mcp/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ DEPLOYMENT.md
â”œâ”€â”€ .env.example
â”œâ”€â”€ .ruff.toml
â”œâ”€â”€ squad-mcp/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # FastAPI + FastMCP entrypoint
â”‚   â”œâ”€â”€ config.py                  # Environment loader & validation
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ auth.py                # Bearer token validation
â”‚   â”‚   â”œâ”€â”€ request_id.py          # Request ID injection
â”‚   â”‚   â””â”€â”€ rate_limit.py          # Rate limiting middleware
â”‚   â”œâ”€â”€ logging_config.py          # JSON logging setup
â”‚   â”œâ”€â”€ metrics.py                 # Prometheus metrics definitions
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rest_tools.py          # REST API wrappers
â”‚   â”‚   â”œâ”€â”€ sql_tools.py           # SQL query tools
â”‚   â”‚   â”œâ”€â”€ rag_tools.py           # Optional RAG tools
â”‚   â”‚   â””â”€â”€ errors.py              # Tool-specific exceptions
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ db.py                  # Safe SQL execution
â”‚       â”œâ”€â”€ api_client.py          # HTTPX wrapper with retries
â”‚       â”œâ”€â”€ health.py              # Health check logic
â”‚       â””â”€â”€ secrets.py             # Secrets rotation helpers
â””â”€â”€ tests/
    â”œâ”€â”€ conftest.py
    â”œâ”€â”€ test_rest_tools.py
    â”œâ”€â”€ test_sql_tools.py
    â”œâ”€â”€ test_auth_middleware.py
    â”œâ”€â”€ test_rate_limiting.py
    â””â”€â”€ test_error_handling.py
```

---

## 7. Implementation Examples

### 7.1 Authentication Middleware
```python
# middleware/auth.py
from fastapi import HTTPException, Request
from fastapi.security import HTTPBearer, HTTPAuthCredentials

async def verify_api_key(request: Request) -> str:
    """Extract and validate Bearer token"""
    auth_header = request.headers.get("Authorization")
    if not auth_header or not auth_header.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Unauthorized")
    
    token = auth_header[7:]
    if token not in VALID_API_KEYS:
        logger.warning(f"Invalid API key attempt", extra={"request_id": request.state.request_id})
        raise HTTPException(status_code=401, detail="Unauthorized")
    
    return token
```

### 7.2 REST Tool with Error Handling
```python
# tools/rest_tools.py
from fastmcp import tool
from pydantic import BaseModel, Field
import httpx
from utils.api_client import call_api_with_retries

class GetCustomerInput(BaseModel):
    customer_id: int = Field(..., gt=0, description="Customer ID")

@tool("get_customer", description="Fetch customer details from REST API")
def get_customer(input: GetCustomerInput) -> dict:
    """Fetch customer with retry logic and timeout handling"""
    try:
        response = call_api_with_retries(
            method="GET",
            url=f"{API_BASE}/customers/{input.customer_id}",
            timeout=10,
            max_retries=3
        )
        return response.json()
    except httpx.TimeoutException:
        logger.error("API timeout", extra={"endpoint": "get_customer", "request_id": request_id})
        return {"error": "API timeout", "status": "retry", "request_id": request_id}
    except httpx.HTTPStatusError as e:
        if e.response.status_code >= 500:
            return {"error": "Upstream error", "status": "retry", "request_id": request_id}
        return {"error": f"Not found", "status": "not_found", "request_id": request_id}
```

### 7.3 SQL Tool with Pagination
```python
# tools/sql_tools.py
from fastmcp import tool
from pydantic import BaseModel, Field
from utils.db import query_db, get_count

class TopCustomersInput(BaseModel):
    limit: int = Field(default=10, ge=1, le=1000, description="Max results")
    offset: int = Field(default=0, ge=0, description="Pagination offset")

@tool("get_top_customers", description="Return top customers by revenue with pagination")
def get_top_customers(input: TopCustomersInput) -> dict:
    """Fetch paginated results with total count"""
    query = '''
        SELECT name, SUM(amount) AS total
        FROM orders
        GROUP BY name
        ORDER BY total DESC
        LIMIT :limit OFFSET :offset
    '''
    try:
        rows = query_db(query, {"limit": input.limit, "offset": input.offset})
        total = get_count("SELECT COUNT(DISTINCT name) FROM orders")
        return {
            "customers": [{"name": r[0], "total": r[1]} for r in rows],
            "limit": input.limit,
            "offset": input.offset,
            "total_count": total
        }
    except Exception as e:
        logger.error("SQL query failed", extra={"query": "get_top_customers", "error": str(e)})
        return {"error": "Query failed", "status": "error", "request_id": request_id}
```

---

## 8. Add Tool Functionality

The purpose of this MCP server is to query the REST API and SQL DB. Analyze the REST API project code and OpenAPI Spec to understand the API endpoints and SQL schemas.

Hypothesize common requests from users based on the types of models in @squad-api/. Add tool calls for those requests, that either query the API or DB.

## 8. Testing Requirements

All code must include:
- Unit tests for all tools (mock upstream dependencies).
- Integration tests for error handling (timeouts, retries, rate limits).
- Security tests (SQL injection resistance, auth validation, input bounds).
- Performance tests (P99 latency under load).
- Minimum 80% code coverage.

Example test patterns provided in template.

---

## 9. Deployment & Scaling

### 9.1 Horizontal Scaling
- Stateless design allows multiple instances behind a load balancer.
- Use environment variables for service discovery (Redis for distributed rate limiting).
- Database connection pooling must account for multiple processes.

### 9.2 Container Orchestration
- Provide `Dockerfile` with health check.
- Use readiness probe at `/health` (all checks passing).
- Use liveness probe at `/health` (database connectivity only).
- Recommended: Kubernetes deployment with auto-scaling on CPU/memory.

### 9.3 Monitoring & Alerting
- Expose `/metrics` for Prometheus scraping.
- Set up alerts for:
  - Error rate > 5% (P(error) > 0.05).
  - P99 latency > 1 second.
  - Health check failures.
  - Rate limit hits spike.
- Integrate with your observability stack (DataDog, New Relic, etc.).

---

## 10. Validation Checklist (Before Deployment)

- [ ] All tool endpoints require Bearer token authentication.
- [ ] Rate limiting enforced at 100 req/min per API key.
- [ ] All errors return JSON (no stack traces to client).
- [ ] Structured JSON logging with request IDs implemented.
- [ ] Prometheus metrics exposed at `/metrics`.
- [ ] `/health` endpoint checks database, upstream API, and memory.
- [ ] All SQL queries use parameterized statements (no f-strings).
- [ ] Pagination enforced on all list endpoints (max 1000 items).
- [ ] HTTP client uses timeouts (10s default) and retry logic.
- [ ] Secrets loaded from `.env`, not hardcoded.
- [ ] Unit tests cover all tools and error paths (80% coverage).
- [ ] Ruff linter passes; no SQL f-string violations.
- [ ] Docker build succeeds and health check passes.
- [ ] API documentation auto-generated at `/docs`.
- [ ] README includes setup, monitoring, scaling, troubleshooting, and security sections.

---

## 11. Recommended Dependencies

```toml
[project]
name = "squad-mcp"
version = "0.1.0"
description = "Production-grade headless MCP Tool Server"
requires-python = ">=3.10"

dependencies = [
    "fastmcp>=0.6.0",
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "httpx>=0.25.0",
    "sqlalchemy>=2.0.0",
    "pydantic>=2.0.0",
    "pydantic-settings>=2.0.0",
    "python-dotenv>=1.0.0",
    "python-json-logger>=2.0.0",
    "prometheus-client>=0.19.0",
    "slowapi>=0.1.9",
    "redis>=5.0.0",  # For distributed rate limiting
]

[tool.uv]
dev-dependencies = [
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.1.0",
    "ruff>=0.1.0",
    "mypy>=1.7.0",
    "httpx[testing]>=0.25.0",
]
```

---

## 12. Examples

You can find the following examples and references if you have trouble:
* **@readme_updated.md**: An example README for an MCP tool fronting an API and a SQL db
* **@implementation_patterns.md**: If you get stuck on any architectural or code structure decisions, refer to this guide for example patterns to common solutions required for this type of workload.

---

## 13. Success Criteria

A completed implementation should:
1. âœ… Start with `uv run squad-mcp.main` and expose `/docs` with all tools documented.
2. âœ… Reject unauthenticated requests with HTTP 401.
3. âœ… Return rate limit errors (HTTP 429) after 100 requests/minute.
4. âœ… Handle upstream timeouts gracefully and retry automatically.
5. âœ… Produce structured JSON logs with request IDs for every operation.
6. âœ… Expose Prometheus metrics at `/metrics`.
7. âœ… Pass all security tests (no SQL injection, no auth bypass, no data leaks).
8. âœ… Scale horizontally with multiple instances.
9. âœ… Have 80%+ test coverage with no linting errors.
10. âœ… Deploy in Docker with passing health checks.
